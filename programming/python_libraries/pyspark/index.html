
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../pandas/">
      
      
        <link rel="next" href="../../sql/introduction/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.3">
    
    
      
        <title>PySpark - Python and data science</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6b71719e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pyspark" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Python and data science" class="md-header__button md-logo" aria-label="Python and data science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Python and data science
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PySpark
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Python and data science" class="md-nav__button md-logo" aria-label="Python and data science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Python and data science
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" tabindex="0" aria-expanded="false">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_introduction/" class="md-nav__link">
        Introduction to Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/control_flow/" class="md-nav__link">
        Control flow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/data_structures/" class="md-nav__link">
        Data structures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/functions/" class="md-nav__link">
        Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_programs/" class="md-nav__link">
        Python programs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/io_files/" class="md-nav__link">
        Input, output and files
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/exceptions/" class="md-nav__link">
        Exceptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/classes/" class="md-nav__link">
        Classes and objects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_packages/" class="md-nav__link">
        Python packages
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3" tabindex="0" aria-expanded="true">
          Data science libraries
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data science libraries" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Data science libraries
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../numpy/" class="md-nav__link">
        Numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../matplotlib/" class="md-nav__link">
        Matplotlib
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pandas/" class="md-nav__link">
        Pandas
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          PySpark
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        PySpark
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#setting-up-spark" class="md-nav__link">
    Setting up Spark
  </a>
  
    <nav class="md-nav" aria-label="Setting up Spark">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ubuntu" class="md-nav__link">
    Ubuntu
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-1-running-a-jupyter-notebook" class="md-nav__link">
    Option 1: running a Jupyter notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-running-pyspark-from-an-ide-eg-pycharm" class="md-nav__link">
    Option 2: running PySpark from an IDE (e.g. PyCharm)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
    <nav class="md-nav" aria-label="Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pyspark-dataframes" class="md-nav__link">
    PySpark DataFrames
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pandas-vs-spark" class="md-nav__link">
    Pandas vs Spark
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-pyspark-dataframes" class="md-nav__link">
    Working with PySpark DataFrames
  </a>
  
    <nav class="md-nav" aria-label="Working with PySpark DataFrames">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataframe-creation" class="md-nav__link">
    DataFrame Creation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" tabindex="0" aria-expanded="false">
          SQL
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="SQL" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          SQL
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/introduction/" class="md-nav__link">
        SQL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/select_statements/" class="md-nav__link">
        SELECT statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/window_functions/" class="md-nav__link">
        Window functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/modification_statements/" class="md-nav__link">
        Data modification statements
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/table_operations/" class="md-nav__link">
        Table operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/sql_from_python/" class="md-nav__link">
        SQL queries from Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#setting-up-spark" class="md-nav__link">
    Setting up Spark
  </a>
  
    <nav class="md-nav" aria-label="Setting up Spark">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ubuntu" class="md-nav__link">
    Ubuntu
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-1-running-a-jupyter-notebook" class="md-nav__link">
    Option 1: running a Jupyter notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#option-2-running-pyspark-from-an-ide-eg-pycharm" class="md-nav__link">
    Option 2: running PySpark from an IDE (e.g. PyCharm)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
    <nav class="md-nav" aria-label="Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pyspark-dataframes" class="md-nav__link">
    PySpark DataFrames
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pandas-vs-spark" class="md-nav__link">
    Pandas vs Spark
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-pyspark-dataframes" class="md-nav__link">
    Working with PySpark DataFrames
  </a>
  
    <nav class="md-nav" aria-label="Working with PySpark DataFrames">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataframe-creation" class="md-nav__link">
    DataFrame Creation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="pyspark">PySpark<a class="headerlink" href="#pyspark" title="Permanent link">&para;</a></h1>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p><strong>Spark</strong> is a platform for cluster computing. Spark lets you spread data and computations over clusters 
with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier 
to work with very large datasets because each node only works with a small amount of data. Hence,
Spark is a common tool for what is informally known as <em>big data</em> (for example, for cleaning it up or transforming
it into a more usable format).</p>
<p>As each node works on its own subset of the total data, it also carries out a part of the total 
calculations required, so that both data processing and computation are performed in parallel over 
the nodes in the cluster. </p>
<p><strong>PySpark</strong> is the Python API for Spark. Having a Python library that allows us to interact with Spark
using Python means that we can use Python to write code that will run on Spark clusters under the hood,
without having to learn Scala or Java.</p>
<h2 id="setting-up-spark">Setting up Spark<a class="headerlink" href="#setting-up-spark" title="Permanent link">&para;</a></h2>
<p>Setting up Spark is considerably more complicated than using Pandas, and it requires additional software. 
We should only use Spark when our data is too big to work with on a single machine. If that is not the case,
we should use Pandas instead (or a Pandas alternative, like Polars).</p>
<h3 id="ubuntu">Ubuntu<a class="headerlink" href="#ubuntu" title="Permanent link">&para;</a></h3>
<p>The easiest way to start using PySpark is with a docker image. We have two options</p>
<h3 id="option-1-running-a-jupyter-notebook">Option 1: running a Jupyter notebook<a class="headerlink" href="#option-1-running-a-jupyter-notebook" title="Permanent link">&para;</a></h3>
<p>Get the docker image from <a href="https://hub.docker.com/r/jupyter/pyspark-notebook">here</a> by running:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>docker<span class="w"> </span>pull<span class="w"> </span>jupyter/pyspark-notebook
</code></pre></div>
Then run the image with
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>-p<span class="w"> </span><span class="m">8888</span>:8888<span class="w"> </span>jupyter/pyspark-notebook
</code></pre></div>
After this two steps we can already write/run Python code with PySpark (from the notebook).</p>
<h3 id="option-2-running-pyspark-from-an-ide-eg-pycharm">Option 2: running PySpark from an IDE (e.g. PyCharm)<a class="headerlink" href="#option-2-running-pyspark-from-an-ide-eg-pycharm" title="Permanent link">&para;</a></h3>
<ol>
<li>Install docker following the instructions <a href="https://docs.docker.com/engine/install/ubuntu/">here</a>.</li>
<li>Pull the docker image following the instructions <a href="https://hub.docker.com/r/apache/spark-py">here</a>.</li>
</ol>
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>At a high level, every Spark application consists of a driver program that runs the user’s main 
function and executes various parallel operations on a cluster. The main abstraction Spark provides 
is a resilient distributed dataset (<strong>RDD</strong>), which is a collection of elements partitioned across the 
nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file 
(Hadoop file system or any other Hadoop-supported file system), or an existing driver program (a Scala collection),
and transforming it. Users may persist an RDD in memory to be reused across parallel operations.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RDDs automatically recover from node failures.</p>
</div>
<h3 id="pyspark-dataframes">PySpark DataFrames<a class="headerlink" href="#pyspark-dataframes" title="Permanent link">&para;</a></h3>
<p>PySpark's main object is the PySpark DataFrame (which is not the same as a Pandas DataFrame). 
PySpark DataFrames are implemented on top of RDDs and are lazily evaluated. 
This means that whenever we create a PySpark DataFrame, nothing happens until we call an action on it:
Spark does not immediately compute the transformation but plans how to compute later. </p>
<p>RDD actions are operations that actually return a value to the user program or write a value to storage,
and actually trigger the computation of a result. They can be:</p>
<ul>
<li><code>collect()</code>: return all the elements of the dataset as an array at the driver program. 
This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</li>
<li><code>count()</code>: return the number of elements in the dataset.</li>
<li><code>first()</code>: return the first element of the dataset.</li>
<li><code>take(n)</code>: return an array with the first n elements of the dataset.</li>
<li><code>reduce(func)</code>: aggregate the elements of the dataset using a function func (which takes two arguments 
and returns one).</li>
</ul>
<h3 id="pandas-vs-spark">Pandas vs Spark<a class="headerlink" href="#pandas-vs-spark" title="Permanent link">&para;</a></h3>
<p>Spark DataFrames are conceptually equivalent to a Pandas DataFrame. The main difference is that
Spark DataFrames are immutable, meaning that they cannot be changed after they are created. This
allows Spark to do more optimization under the hood.</p>
<h2 id="working-with-pyspark-dataframes">Working with PySpark DataFrames<a class="headerlink" href="#working-with-pyspark-dataframes" title="Permanent link">&para;</a></h2>
<h3 id="dataframe-creation">DataFrame Creation<a class="headerlink" href="#dataframe-creation" title="Permanent link">&para;</a></h3>
<p>A PySpark DataFrame can be created via <code>pyspark.sql.SparkSession.createDataFrame</code> typically by passing a list 
of lists, tuples, dictionaries and pyspark.sql.Rows, a pandas DataFrame and an RDD consisting of such a list. 
<code>pyspark.sql.SparkSession.createDataFrame</code> takes the schema argument to specify the schema of the DataFrame.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A PySpark schema is a list that defines the name, type, and nullable/non-nullable information for each column
in a DataFrame.</p>
</div>
<p>When the schema is omitted, PySpark infers the corresponding schema by taking a sample from the data.</p>
<p>Firstly, you can create a PySpark DataFrame from a list of rows</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">date</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string1&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string2&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string3&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="p">])</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="n">df</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="n">DataFrame</span><span class="p">[</span><span class="n">a</span><span class="p">:</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">double</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">string</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">timestamp</span><span class="p">]</span>
</code></pre></div>
<p>Create a PySpark DataFrame with an explicit schema.</p>
<p>df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a long, b double, c string, d date, e timestamp')
df</p>
<p>DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]</p>
<p>Create a PySpark DataFrame from a pandas DataFrame</p>
<p>pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
df</p>
<p>DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]</p>
<p>The DataFrames created above all have the same results and schema.</p>
<h1 id="all-dataframes-above-result-same">All DataFrames above result same.<a class="headerlink" href="#all-dataframes-above-result-same" title="Permanent link">&para;</a></h1>
<p>df.show()
df.printSchema()</p>
<p>+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|
+---+---+-------+----------+-------------------+</p>
<p>root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: date (nullable = true)
 |-- e: timestamp (nullable = true)</p>
<p>Viewing Data</p>
<p>The top rows of a DataFrame can be displayed using DataFrame.show().</p>
<p>df.show(1)</p>
<p>+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
+---+---+-------+----------+-------------------+
only showing top 1 row</p>
<p>Alternatively, you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration.</p>
<p>spark.conf.set('spark.sql.repl.eagerEval.enabled', True)
df</p>
<p>a   b   c   d   e
1   2.0 string1 2000-01-01  2000-01-01 12:00:00
2   3.0 string2 2000-02-01  2000-01-02 12:00:00
3   4.0 string3 2000-03-01  2000-01-03 12:00:00</p>
<p>The rows can also be shown vertically. This is useful when rows are too long to show horizontally.</p>
<p>df.show(1, vertical=True)</p>
<p>-RECORD 0------------------
 a   | 1
 b   | 2.0
 c   | string1
 d   | 2000-01-01
 e   | 2000-01-01 12:00:00
only showing top 1 row</p>
<p>You can see the DataFrame’s schema and column names as follows:</p>
<p>df.columns</p>
<p>['a', 'b', 'c', 'd', 'e']</p>
<p>df.printSchema()</p>
<p>root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: date (nullable = true)
 |-- e: timestamp (nullable = true)</p>
<p>Show the summary of the DataFrame</p>
<p>df.select("a", "b", "c").describe().show()</p>
<p>+-------+---+---+-------+
|summary|  a|  b|      c|
+-------+---+---+-------+
|  count|  3|  3|      3|
|   mean|2.0|3.0|   null|
| stddev|1.0|1.0|   null|
|    min|  1|2.0|string1|
|    max|  3|4.0|string3|
+-------+---+---+-------+</p>
<p>DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.</p>
<p>df.collect()</p>
<p>[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),
 Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),
 Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]</p>
<p>In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail().</p>
<p>df.take(1)</p>
<p>[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]</p>
<p>PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side.</p>
<p>df.toPandas()</p>
<div class="highlight"><pre><span></span><code>a   b   c   d   e
</code></pre></div>
<p>0   1   2.0     string1     2000-01-01  2000-01-01 12:00:00
1   2   3.0     string2     2000-02-01  2000-01-02 12:00:00
2   3   4.0     string3     2000-03-01  2000-01-03 12:00:00
Selecting and Accessing Data</p>
<p>PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance.</p>
<p>df.a</p>
<p>Column<b'a'></p>
<p>In fact, most of column-wise operations return Columns.</p>
<p>from pyspark.sql import Column
from pyspark.sql.functions import upper</p>
<p>type(df.c) == type(upper(df.c)) == type(df.c.isNull())</p>
<p>True</p>
<p>These Columns can be used to select the columns from a DataFrame. For example, DataFrame.select() takes the Column instances that returns another DataFrame.</p>
<p>df.select(df.c).show()</p>
<p>+-------+
|      c|
+-------+
|string1|
|string2|
|string3|
+-------+</p>
<p>Assign new Column instance.</p>
<p>df.withColumn('upper_c', upper(df.c)).show()</p>
<p>+---+---+-------+----------+-------------------+-------+
|  a|  b|      c|         d|                  e|upper_c|
+---+---+-------+----------+-------------------+-------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|
|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|
+---+---+-------+----------+-------------------+-------+</p>
<p>To select a subset of rows, use DataFrame.filter().</p>
<p>df.filter(df.a == 1).show()</p>
<p>+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
+---+---+-------+----------+-------------------+</p>
<p>Applying a Function</p>
<p>PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest Pandas UDFs and Pandas Function APIs. For instance, the example below allows users to directly use the APIs in a pandas Series within Python native function.</p>
<p>import pandas as pd
from pyspark.sql.functions import pandas_udf</p>
<p>@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -&gt; pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1</p>
<p>df.select(pandas_plus_one(df.a)).show()</p>
<p>+------------------+
|pandas_plus_one(a)|
+------------------+
|                 2|
|                 3|
|                 4|
+------------------+</p>
<p>Another example is DataFrame.mapInPandas which allows users directly use the APIs in a pandas DataFrame without any restrictions such as the result length.</p>
<p>def pandas_filter_func(iterator):
    for pandas_df in iterator:
        yield pandas_df[pandas_df.a == 1]</p>
<p>df.mapInPandas(pandas_filter_func, schema=df.schema).show()</p>
<p>+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
+---+---+-------+----------+-------------------+</p>
<p>Grouping Data</p>
<p>PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame.</p>
<p>df = spark.createDataFrame([
    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],
    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],
    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])
df.show()</p>
<p>+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana|  1| 10|
| blue|banana|  2| 20|
|  red|carrot|  3| 30|
| blue| grape|  4| 40|
|  red|carrot|  5| 50|
|black|carrot|  6| 60|
|  red|banana|  7| 70|
|  red| grape|  8| 80|
+-----+------+---+---+</p>
<p>Grouping and then applying the avg() function to the resulting groups.</p>
<p>df.groupby('color').avg().show()</p>
<p>+-----+-------+-------+
|color|avg(v1)|avg(v2)|
+-----+-------+-------+
|  red|    4.8|   48.0|
|black|    6.0|   60.0|
| blue|    3.0|   30.0|
+-----+-------+-------+</p>
<p>You can also apply a Python native function against each group by using pandas API.</p>
<p>def plus_mean(pandas_df):
    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())</p>
<p>df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()</p>
<p>+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana| -3| 10|
|  red|carrot| -1| 30|
|  red|carrot|  0| 50|
|  red|banana|  2| 70|
|  red| grape|  3| 80|
|black|carrot|  0| 60|
| blue|banana| -1| 20|
| blue| grape|  1| 40|
+-----+------+---+---+</p>
<p>Co-grouping and applying a function.</p>
<p>df1 = spark.createDataFrame(
    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],
    ('time', 'id', 'v1'))</p>
<p>df2 = spark.createDataFrame(
    [(20000101, 1, 'x'), (20000101, 2, 'y')],
    ('time', 'id', 'v2'))</p>
<p>def merge_ordered(l, r):
    return pd.merge_ordered(l, r)</p>
<p>df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(
    merge_ordered, schema='time int, id int, v1 double, v2 string').show()</p>
<p>+--------+---+---+---+
|    time| id| v1| v2|
+--------+---+---+---+
|20000101|  1|1.0|  x|
|20000102|  1|3.0|  x|
|20000101|  2|2.0|  y|
|20000102|  2|4.0|  y|
+--------+---+---+---+</p>
<p>Getting Data In/Out</p>
<p>CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster.</p>
<p>There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest Spark SQL, DataFrames and Datasets Guide in Apache Spark documentation.
CSV</p>
<p>df.write.csv('foo.csv', header=True)
spark.read.csv('foo.csv', header=True).show()</p>
<p>+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana|  1| 10|
| blue|banana|  2| 20|
|  red|carrot|  3| 30|
| blue| grape|  4| 40|
|  red|carrot|  5| 50|
|black|carrot|  6| 60|
|  red|banana|  7| 70|
|  red| grape|  8| 80|
+-----+------+---+---+</p>
<p>Parquet</p>
<p>df.write.parquet('bar.parquet')
spark.read.parquet('bar.parquet').show()</p>
<p>+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana|  1| 10|
| blue|banana|  2| 20|
|  red|carrot|  3| 30|
| blue| grape|  4| 40|
|  red|carrot|  5| 50|
|black|carrot|  6| 60|
|  red|banana|  7| 70|
|  red| grape|  8| 80|
+-----+------+---+---+</p>
<p>ORC</p>
<p>df.write.orc('zoo.orc')
spark.read.orc('zoo.orc').show()</p>
<p>+-----+------+---+---+
|color| fruit| v1| v2|
+-----+------+---+---+
|  red|banana|  1| 10|
| blue|banana|  2| 20|
|  red|carrot|  3| 30|
| blue| grape|  4| 40|
|  red|carrot|  5| 50|
|black|carrot|  6| 60|
|  red|banana|  7| 70|
|  red| grape|  8| 80|
+-----+------+---+---+</p>
<p>Working with SQL</p>
<p>DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:</p>
<p>df.createOrReplaceTempView("tableA")
spark.sql("SELECT count(*) from tableA").show()</p>
<p>+--------+
|count(1)|
+--------+
|       8|
+--------+</p>
<p>In addition, UDFs can be registered and invoked in SQL out of the box:</p>
<p>@pandas_udf("integer")
def add_one(s: pd.Series) -&gt; pd.Series:
    return s + 1</p>
<p>spark.udf.register("add_one", add_one)
spark.sql("SELECT add_one(v1) FROM tableA").show()</p>
<p>+-----------+
|add_one(v1)|
+-----------+
|          2|
|          3|
|          4|
|          5|
|          6|
|          7|
|          8|
|          9|
+-----------+</p>
<p>These SQL expressions can directly be mixed and used as PySpark columns.</p>
<p>from pyspark.sql.functions import expr</p>
<p>df.selectExpr('add_one(v1)').show()
df.select(expr('count(*)') &gt; 0).show()</p>
<p>+-----------+
|add_one(v1)|
+-----------+
|          2|
|          3|
|          4|
|          5|
|          6|
|          7|
|          8|
|          9|
+-----------+</p>
<p>+--------------+
|(count(1) &gt; 0)|
+--------------+
|          true|
+--------------+</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.ba449ae6.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>