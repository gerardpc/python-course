
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../other/git/">
      
      
        <link rel="next" href="../typer/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.3">
    
    
      
        <title>Scikit Learn - Python and data science</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6b71719e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#scikit-learn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Python and data science" class="md-header__button md-logo" aria-label="Python and data science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Python and data science
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Scikit Learn
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Python and data science" class="md-nav__button md-logo" aria-label="Python and data science" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Python and data science
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" tabindex="0" aria-expanded="false">
          Python 1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python 1" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Python 1
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_introduction/" class="md-nav__link">
        Introduction to Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/control_flow/" class="md-nav__link">
        Control flow
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/data_structures/" class="md-nav__link">
        Data structures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/functions/" class="md-nav__link">
        Functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_programs/" class="md-nav__link">
        Python programs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/io_files/" class="md-nav__link">
        Input, output and files
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/exceptions/" class="md-nav__link">
        Exceptions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/classes/" class="md-nav__link">
        Classes and objects
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
        <label class="md-nav__link" for="__nav_3" tabindex="0" aria-expanded="true">
          Python 2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python 2" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Python 2
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/datetime/" class="md-nav__link">
        Dates and times
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/regular_expressions/" class="md-nav__link">
        Regular expressions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../sqlalchemy/" class="md-nav__link">
        Connecting SQL to Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/python_projects/" class="md-nav__link">
        Python projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../other/git/" class="md-nav__link">
        Git
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Scikit Learn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Scikit Learn
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    Datasets
  </a>
  
    <nav class="md-nav" aria-label="Datasets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-vs-unsupervised-problems" class="md-nav__link">
    Supervised vs Unsupervised problems
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scikit-learns-api" class="md-nav__link">
    Scikit-Learn's API
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#estimators-api" class="md-nav__link">
    Estimators' API
  </a>
  
    <nav class="md-nav" aria-label="Estimators' API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-fit-method" class="md-nav__link">
    The fit method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-predict-method" class="md-nav__link">
    The predict method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    Transformers
  </a>
  
    <nav class="md-nav" aria-label="Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-scaler" class="md-nav__link">
    Standard scaler
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#one-hot-encoder" class="md-nav__link">
    One-hot encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ordinal-encoder" class="md-nav__link">
    Ordinal encoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metrics" class="md-nav__link">
    Metrics
  </a>
  
    <nav class="md-nav" aria-label="Metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-metrics" class="md-nav__link">
    Classification metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-metrics" class="md-nav__link">
    Regression metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-validation" class="md-nav__link">
    Model validation
  </a>
  
    <nav class="md-nav" aria-label="Model validation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#holdout-sets-with-train_test_split" class="md-nav__link">
    Holdout sets with train_test_split
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    Cross-validation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-hyperparameters" class="md-nav__link">
    Model hyperparameters
  </a>
  
    <nav class="md-nav" aria-label="Model hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance-trade-off" class="md-nav__link">
    Bias-variance trade-off
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation-curves" class="md-nav__link">
    Validation curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-curves" class="md-nav__link">
    Learning curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation-including-hyperparameter-tuning" class="md-nav__link">
    Cross-validation including hyperparameter tuning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-engineering" class="md-nav__link">
    Feature Engineering
  </a>
  
    <nav class="md-nav" aria-label="Feature Engineering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-features" class="md-nav__link">
    Types of features
  </a>
  
    <nav class="md-nav" aria-label="Types of features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#categorical-features" class="md-nav__link">
    Categorical features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-features" class="md-nav__link">
    Text features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#date-features" class="md-nav__link">
    Date features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-features" class="md-nav__link">
    Image features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derived-features" class="md-nav__link">
    Derived features
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-selection" class="md-nav__link">
    Feature selection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#machine-learning-algorithms-in-sklearn" class="md-nav__link">
    Machine learning algorithms in SKLearn
  </a>
  
    <nav class="md-nav" aria-label="Machine learning algorithms in SKLearn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression-models" class="md-nav__link">
    Regression models
  </a>
  
    <nav class="md-nav" aria-label="Regression models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-regression-models" class="md-nav__link">
    Common regression models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-examples" class="md-nav__link">
    Regression examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav" aria-label="Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-classification-models" class="md-nav__link">
    Common classification models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering" class="md-nav__link">
    Clustering
  </a>
  
    <nav class="md-nav" aria-label="Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-clustering-models" class="md-nav__link">
    Common clustering models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_1" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensionality-reduction" class="md-nav__link">
    Dimensionality reduction
  </a>
  
    <nav class="md-nav" aria-label="Dimensionality reduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-dimensionality-reduction-models" class="md-nav__link">
    Common dimensionality reduction models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_2" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#annex-common-pitfalls-in-ml" class="md-nav__link">
    Annex: common pitfalls in ML
  </a>
  
    <nav class="md-nav" aria-label="Annex: common pitfalls in ML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    Curse of dimensionality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting" class="md-nav__link">
    Overfitting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#underfitting" class="md-nav__link">
    Underfitting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-leakage" class="md-nav__link">
    Data leakage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-imbalance" class="md-nav__link">
    Class imbalance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other" class="md-nav__link">
    Other
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../typer/" class="md-nav__link">
        Typer
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" tabindex="0" aria-expanded="false">
          Other Python topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Other Python topics" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Other Python topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../python/comprehensions/" class="md-nav__link">
        Comprehensions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" tabindex="0" aria-expanded="false">
          Python data libraries
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python data libraries" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Python data libraries
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../numpy/" class="md-nav__link">
        Numpy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../matplotlib/" class="md-nav__link">
        Matplotlib
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pandas/" class="md-nav__link">
        Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pyspark/" class="md-nav__link">
        PySpark
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" tabindex="0" aria-expanded="false">
          SQL
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="SQL" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          SQL
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/introduction/" class="md-nav__link">
        SQL
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/select_statements/" class="md-nav__link">
        SELECT statement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/window_functions/" class="md-nav__link">
        Window functions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/modification_statements/" class="md-nav__link">
        Data modification statements
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/table_operations/" class="md-nav__link">
        Table operations
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../sql/sql_from_python/" class="md-nav__link">
        Connecting SQL to Python
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    Datasets
  </a>
  
    <nav class="md-nav" aria-label="Datasets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-vs-unsupervised-problems" class="md-nav__link">
    Supervised vs Unsupervised problems
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scikit-learns-api" class="md-nav__link">
    Scikit-Learn's API
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#estimators-api" class="md-nav__link">
    Estimators' API
  </a>
  
    <nav class="md-nav" aria-label="Estimators' API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-fit-method" class="md-nav__link">
    The fit method
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-predict-method" class="md-nav__link">
    The predict method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    Transformers
  </a>
  
    <nav class="md-nav" aria-label="Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-scaler" class="md-nav__link">
    Standard scaler
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#one-hot-encoder" class="md-nav__link">
    One-hot encoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ordinal-encoder" class="md-nav__link">
    Ordinal encoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metrics" class="md-nav__link">
    Metrics
  </a>
  
    <nav class="md-nav" aria-label="Metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-metrics" class="md-nav__link">
    Classification metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-metrics" class="md-nav__link">
    Regression metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-validation" class="md-nav__link">
    Model validation
  </a>
  
    <nav class="md-nav" aria-label="Model validation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#holdout-sets-with-train_test_split" class="md-nav__link">
    Holdout sets with train_test_split
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    Cross-validation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-hyperparameters" class="md-nav__link">
    Model hyperparameters
  </a>
  
    <nav class="md-nav" aria-label="Model hyperparameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bias-variance-trade-off" class="md-nav__link">
    Bias-variance trade-off
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#validation-curves" class="md-nav__link">
    Validation curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-curves" class="md-nav__link">
    Learning curves
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation-including-hyperparameter-tuning" class="md-nav__link">
    Cross-validation including hyperparameter tuning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-engineering" class="md-nav__link">
    Feature Engineering
  </a>
  
    <nav class="md-nav" aria-label="Feature Engineering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-features" class="md-nav__link">
    Types of features
  </a>
  
    <nav class="md-nav" aria-label="Types of features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#categorical-features" class="md-nav__link">
    Categorical features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-features" class="md-nav__link">
    Text features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#date-features" class="md-nav__link">
    Date features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-features" class="md-nav__link">
    Image features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derived-features" class="md-nav__link">
    Derived features
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feature-selection" class="md-nav__link">
    Feature selection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#machine-learning-algorithms-in-sklearn" class="md-nav__link">
    Machine learning algorithms in SKLearn
  </a>
  
    <nav class="md-nav" aria-label="Machine learning algorithms in SKLearn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression-models" class="md-nav__link">
    Regression models
  </a>
  
    <nav class="md-nav" aria-label="Regression models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-regression-models" class="md-nav__link">
    Common regression models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-examples" class="md-nav__link">
    Regression examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav" aria-label="Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-classification-models" class="md-nav__link">
    Common classification models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering" class="md-nav__link">
    Clustering
  </a>
  
    <nav class="md-nav" aria-label="Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-clustering-models" class="md-nav__link">
    Common clustering models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_1" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimensionality-reduction" class="md-nav__link">
    Dimensionality reduction
  </a>
  
    <nav class="md-nav" aria-label="Dimensionality reduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-dimensionality-reduction-models" class="md-nav__link">
    Common dimensionality reduction models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examples_2" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#annex-common-pitfalls-in-ml" class="md-nav__link">
    Annex: common pitfalls in ML
  </a>
  
    <nav class="md-nav" aria-label="Annex: common pitfalls in ML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    Curse of dimensionality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overfitting" class="md-nav__link">
    Overfitting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#underfitting" class="md-nav__link">
    Underfitting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-leakage" class="md-nav__link">
    Data leakage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-imbalance" class="md-nav__link">
    Class imbalance
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other" class="md-nav__link">
    Other
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="scikit-learn">Scikit Learn<a class="headerlink" href="#scikit-learn" title="Permanent link">&para;</a></h1>
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>There are several Python libraries which provide implementations of a range of machine learning algorithms. 
One of the best known is <strong>Scikit-Learn</strong>, a package that provides efficient versions of a large number of common 
algorithms. Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very 
useful and complete online documentation. A benefit of this uniformity is that once you understand the basic 
use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.</p>
<p>Scikit learn is built on top of NumPy, SciPy, and matplotlib. It contains a tools for classical machine learning and
statistical modeling, including classification, regression, clustering and dimensionality 
reduction. It is designed to interoperate with the Python numerical and scientific libraries 
NumPy and SciPy (but <em>not</em> Pandas by default). In addition, it also provides a selection of sample datasets,
most of which are fairly small and well-known.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scikit Learn is not designed to create neural networks / work with deep learning. For this 
the most popular libraries are PyTorch and TensorFlow + Keras.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scikit Learn is a very large library, so we will only cover the basics here. For more information,
check the <a href="https://scikit-learn.org/stable/">official documentation</a>.</p>
</div>
<h2 id="installation">Installation<a class="headerlink" href="#installation" title="Permanent link">&para;</a></h2>
<p>To install Scikit Learn, we can use <code>pip</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>pip<span class="w"> </span>install<span class="w"> </span>scikit-learn
</code></pre></div>
<p>or, alternatively, <code>poetry</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>poetry<span class="w"> </span>add<span class="w"> </span>scikit-learn
</code></pre></div>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be 
represented in order to be understood by the computer. The best way to think about data within Scikit-Learn is 
in terms of tables of data.</p>
<h3 id="datasets">Datasets<a class="headerlink" href="#datasets" title="Permanent link">&para;</a></h3>
<p>A basic table is a two-dimensional grid of data, in which the rows represent individual elements of the 
dataset, and the columns represent quantities related to each of these elements. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scikit Learn contains several datasets that can be used to test machine learning algorithms.
To load a dataset, we can use the <code>load_*</code> functions, where <code>*</code> is the name of the dataset (e.g. <code>load_iris</code>).</p>
</div>
<p>For example, consider the Iris dataset, famously analyzed by Ronald Fisher in 1936:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">iris_df</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">frame</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">iris_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="c1"># Output:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    <span class="n">sepal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>  <span class="n">sepal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>  <span class="n">petal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>  <span class="n">petal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>  <span class="n">target</span> <span class="n">target_names</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="mi">0</span>                 <span class="mf">5.1</span>               <span class="mf">3.5</span>                <span class="mf">1.4</span>               <span class="mf">0.2</span>       <span class="mi">0</span>       <span class="n">setosa</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span class="mi">1</span>                 <span class="mf">4.9</span>               <span class="mf">3.0</span>                <span class="mf">1.4</span>               <span class="mf">0.2</span>       <span class="mi">0</span>       <span class="n">setosa</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a><span class="mi">2</span>                 <span class="mf">4.7</span>               <span class="mf">3.2</span>                <span class="mf">1.3</span>               <span class="mf">0.2</span>       <span class="mi">0</span>       <span class="n">setosa</span>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="mi">3</span>                 <span class="mf">4.6</span>               <span class="mf">3.1</span>                <span class="mf">1.5</span>               <span class="mf">0.2</span>       <span class="mi">0</span>       <span class="n">setosa</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="mi">4</span>                 <span class="mf">5.0</span>               <span class="mf">3.6</span>                <span class="mf">1.4</span>               <span class="mf">0.2</span>       <span class="mi">0</span>       <span class="n">setosa</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>as_frame=True</code> argument tells Scikit Learn to return a Pandas DataFrame instead of a NumPy array.
This is useful because we can use the column names to refer to the columns, instead of using the column indices.</p>
</div>
<p>After calling the <code>load_iris()</code> function, we get the data from the Iris dataset. There are typically two
ways to return the data:</p>
<ul>
<li>
<p>As <code>X</code>, <code>y</code> arrays. Here <code>X</code> is the data (independent variables) and <code>y</code> is the target (dependent variables).
  This option can be chosen by setting <code>return_X_y=True</code> when calling the load method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>X</code> and <code>y</code> names are very general and, in the context of machine learning, almost always refer to the
data and target variables, respectively (not only in Scikit Learn, but in other libraries as well). </p>
</div>
</li>
<li>
<p>The other option is a <code>Bunch</code> object, which is similar to a dictionary (which is what we used in the previous
  example). It contains the data, the target, and other information about the dataset:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="c1"># Output:</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="n">dict_keys</span><span class="p">([</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;frame&#39;</span><span class="p">,</span> <span class="s1">&#39;target_names&#39;</span><span class="p">,</span> <span class="s1">&#39;DESCR&#39;</span><span class="p">,</span> <span class="s1">&#39;feature_names&#39;</span><span class="p">,</span> <span class="s1">&#39;filename&#39;</span><span class="p">])</span>
</code></pre></div>
<p>The values of this dictionary are, by default, NumPy arrays, except if we used <code>as_frame=True</code> when calling
the load method, in which case they are Pandas DataFrames:</p>
<ul>
<li><code>data</code>: the data, which is a NumPy array or a Pandas DataFrame. It contains the features of the dataset (i.e,
the independent variables). In this table, each column is a feature, and each row is an observation.</li>
<li><code>target</code>: the target, which is a NumPy array or a Pandas DataFrame. It contains the labels of the dataset (i.e,
  the dependent variable).</li>
<li><code>frame</code>: the data and target combined into a single DataFrame.</li>
<li><code>target_names</code>: the names of the labels (in this case, the names of the flower species).</li>
<li><code>DESCR</code>: the description of the dataset.</li>
<li><code>feature_names</code>: the names of the features (in this case, the names of the flower measurements).</li>
<li><code>filename</code>: the path to the file containing the dataset.</li>
</ul>
</li>
</ul>
<h4 id="supervised-vs-unsupervised-problems">Supervised vs Unsupervised problems<a class="headerlink" href="#supervised-vs-unsupervised-problems" title="Permanent link">&para;</a></h4>
<p>When using scikit-learn, we will typically work with tables following this <code>data</code> and <code>target</code> keys convention.
This is always the case when we are working with <strong>supervised learning</strong> machine learning problems, where we have a
target variable that we want to predict. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Supervised problems</strong> are, at a very high level, interpolation problems. We have a set of points, and we want to
find a function <span class="arithmatex">\(f(X)\)</span> that passes through all of them, so that <span class="arithmatex">\(f(X) \sim y\)</span>. </p>
<p>More formally, we want to minimize the error (known as the loss or cost function), </p>
<div class="arithmatex">\[
\min_{f} \mathcal{L}(f(X), y)
\]</div>
<p>By doing this, we hope that if we use the function in a new point <span class="arithmatex">\(X'\)</span>, we will get a value <span class="arithmatex">\(f(X')\)</span> that is close to
the true value of its associated <span class="arithmatex">\(y'\)</span>.</p>
</div>
<p>In the Iris dataset, the target variable is the flower species, and the features are the flower measurements, so
if were to use this dataset for a machine learning problem, we would try to construct a model that predicts the 
flower species (dependent variable) from the flower measurements (independent variables).</p>
<p>When we are working with <strong>unsupervised learning</strong> machine learning problems we don't have a target variable
that we want to predict. In such cases, we will usually work with tables following the <code>data</code> <em>only</em> key convention, 
since we don't have a target variable. </p>
<h3 id="scikit-learns-api">Scikit-Learn's API<a class="headerlink" href="#scikit-learns-api" title="Permanent link">&para;</a></h3>
<p>Scikit-Learn's API is designed around the following principles:</p>
<ul>
<li><strong>Consistency</strong>: All objects share a common interface drawn from a limited set of methods, with consistent
  documentation. This means that we can learn one estimator API (i.e., methods) and other estimators will work the
  same way.</li>
<li><strong>Inspection</strong>: All parameter values are exposed as public attributes. This means that we can inspect
  the parameters of an estimator by looking at its attributes.</li>
<li><strong>Limited object hierarchy</strong>: Only algorithms are represented by Python classes; datasets are represented in
  standard formats (NumPy arrays, Pandas DataFrames, SciPy sparse matrices) and parameter names use standard Python
  strings.</li>
<li><strong>Composition</strong>: Many machine learning tasks can be expressed as sequences of more fundamental algorithms, and
  Scikit-Learn makes use of this wherever possible. </li>
<li><strong>Sensible defaults</strong>: When models require user-specified parameters, the library defines an appropriate default
  value.</li>
</ul>
<p>In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood.   </p>
<h2 id="estimators-api">Estimators' API<a class="headerlink" href="#estimators-api" title="Permanent link">&para;</a></h2>
<p><strong>Estimator</strong> are classes from the library that implement machine learning algorithms (i.e., objects that "learn"
from data). In the context of Supervised Learning, an estimator is a Python object that implements (at least) the 
following methods:</p>
<ul>
<li><code>fit(X, y)</code>: fit the model using <code>X</code> as training data and <code>y</code> as target values.</li>
<li><code>predict(X)</code>: predict the target values of <code>X</code> using the trained model.</li>
</ul>
<p>If the prediction task is to classify the observations in a set of finite labels, in other words to “name” 
the objects observed, the task is said to be a <em>classification</em> task. In this case, the target variable is said to be
a <em>categorical</em> variable (i.e., a variable that can take on one of a limited, and usually fixed, number of possible
values). If, on the other hand, the target variable is a continuous target variable, it is said to be a 
<em>regression</em> task.</p>
<h3 id="the-fit-method">The <code>fit</code> method<a class="headerlink" href="#the-fit-method" title="Permanent link">&para;</a></h3>
<p>The <code>fit</code> method is used to train the model, and is the first method that we should call when using an estimator (after
our data is ready, of course). It takes as input the training data and the training labels, and
trains the model. For example, to train a KNN classifier, we can do the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A K-Nearest Neightbors is an algorithm that does not fit a model to the data. Instead, it memorizes the training
data in an internal data structure that is efficient to query. However, the example serves as an illustration that
in Scikit Learn, the order is always the same: first we create the estimator, then we fit it to the data, then we
use it to make predictions.</p>
</div>
<h3 id="the-predict-method">The <code>predict</code> method<a class="headerlink" href="#the-predict-method" title="Permanent link">&para;</a></h3>
<p>The <code>predict</code> method is used to make predictions. It takes as input the data to predict, and returns the predictions.
For example, to predict the labels of a test set, we can do the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">X_test</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h2 id="transformers">Transformers<a class="headerlink" href="#transformers" title="Permanent link">&para;</a></h2>
<p>SciKit learn also provides classes known as <strong>transformers</strong>, which are estimators (i.e., they inherit from a
base class called <code>BaseEstimator</code>) that can transform data. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These transformers have nothing to do with the transformers in deep learning. </p>
</div>
<p>Transformers are typically used to preprocess the data before training the model. We'll discuss some examples
in the following sections, but bear in mind that many of these transformations can be done by hand, for example
in Pandas, before going into Scikit Learn.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Transformers are estimators, so they also have <code>fit</code> and <code>predict</code> methods. However, the <code>fit</code> method is used to
learn the parameters of the transformer, and the <code>predict</code> method is used to transform the data. Hence, these
methods are not used in the same way as in estimators, which represent machine learning algorithms.</p>
</div>
<h3 id="standard-scaler">Standard scaler<a class="headerlink" href="#standard-scaler" title="Permanent link">&para;</a></h3>
<p>The <code>StandardScaler</code> is a transformer used to standardize the data before training a model. 
It removes the mean of the data and scales it unit variance, calculating the z-score of each sample in the dataset:
$$
z = \frac{x - u}{s}
$$
where <code>u</code> is the mean of the training samples or zero if <code>with_mean=False</code>, and <code>s</code> is the standard deviation of 
the training samples or one if <code>with_std=False</code>:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>fit_transform</code> method is a combination of the <code>fit</code> and <code>transform</code> methods. It first fits the transformer
to the data (which in this case, although confusing, means computing the mean and std), and then transforms the 
data. This is equivalent to calling <code>fit</code> and then <code>transform</code> separately.</p>
</div>
<h3 id="one-hot-encoder">One-hot encoder<a class="headerlink" href="#one-hot-encoder" title="Permanent link">&para;</a></h3>
<p>The <code>OneHotEncoder</code> is a transformer used to encode categorical features that do not have a natural ordering 
as a one-hot numeric array (i.e., a binary array with a single 1 and many 0s). 
For example, if we have a categorical feature with three possible values, <code>a</code>, <code>b</code>, and <code>c</code>, the one-hot encoding
would be:</p>
<table>
<thead>
<tr>
<th>a</th>
<th>b</th>
<th>c</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">]})</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a><span class="nb">print</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="c1"># Output:</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a><span class="p">[[</span><span class="mf">1.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a> <span class="p">[</span><span class="mf">0.</span> <span class="mf">1.</span> <span class="mf">0.</span><span class="p">]</span>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a> <span class="p">[</span><span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">1.</span><span class="p">]</span>
</code></pre></div>
<h3 id="ordinal-encoder">Ordinal encoder<a class="headerlink" href="#ordinal-encoder" title="Permanent link">&para;</a></h3>
<p>The <code>OrdinalEncoder</code> is a transformer used to encode categorical features that have a natural ordering as a numeric
array. For example, if we have a categorical feature with three possible values that have a meaningful order,
e.g. <code>low</code>, <code>medium</code>, and <code>high</code>, the ordinal encoding would be:</p>
<table>
<thead>
<tr>
<th>low</th>
<th>medium</th>
<th>high</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;low&#39;</span><span class="p">,</span> <span class="s1">&#39;medium&#39;</span><span class="p">,</span> <span class="s1">&#39;high&#39;</span><span class="p">]})</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="nb">print</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">)</span>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="c1"># Output:</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a><span class="p">[[</span><span class="mf">0.</span><span class="p">]</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a> <span class="p">[</span><span class="mf">1.</span><span class="p">]</span>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a> <span class="p">[</span><span class="mf">2.</span><span class="p">]]</span>
</code></pre></div>
<h2 id="metrics">Metrics<a class="headerlink" href="#metrics" title="Permanent link">&para;</a></h2>
<p>A <strong>metric</strong> is a function that measures the performance of a model. For example, the accuracy is a metric 
that measures the proportion of correct predictions made by a classifier, and the mean squared error is a metric
that measures the average squared difference between the predicted values and the true values of a regression model.</p>
<p>Scikit Learn provides a number of metrics that can be used to evaluate the performance of a model. These metrics,
or modified versions of it, are used as the loss function in the optimization problems that the models are trying 
to solve during training (i.e., when you call the <code>fit</code> method).</p>
<p>The Scikit learn API provides two ways to use metrics:</p>
<ul>
<li><strong>The <code>score</code> method</strong>: The <code>score</code> method is a method that is implemented by all estimators. It takes as input
  the data and the target, and returns a score that measures the performance of the model. The score is usually
  a number between 0 and 1, where 1 means that the model is perfect and 0 means that the model is useless. Check
  the documentation of the estimator to see what score it returns.</li>
<li><strong>The <code>metrics</code> module</strong>: The <code>metrics</code> module contains a number of functions that can be used to evaluate the
  performance of a model. These functions take as input the true values and the predicted values, and return a score
  that measures the performance of the model. The score is usually a number between 0 and 1, where 1 means that the
  model is perfect and 0 means that the model is useless. </li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, it is <strong>not possible</strong> to use a custom loss function in Scikit Learn. For example, if you want to use
a different metric than vanilla mean squared error in a regression problem, you should take a look at other
estimators that implement other loss functions. Take a look <a href="https://scikit-learn.org/stable/modules/linear_model.html">here</a>.</p>
</div>
<h3 id="classification-metrics">Classification metrics<a class="headerlink" href="#classification-metrics" title="Permanent link">&para;</a></h3>
<p>Some of the most common classification metrics are:</p>
<ul>
<li>
<p><strong>Accuracy</strong>: The accuracy is the proportion of correct predictions made by a classifier. It is defined as:
  $$
  \text{accuracy} = \frac{\text{number of correct predictions}}{\text{total number of predictions}}
  $$
  The accuracy is a number between 0 and 1, where 1 means that the model is perfect and 0 means that the model is
  useless. The accuracy is implemented by all classifiers in Scikit Learn, and can be often computed using the <code>score</code>
  method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The accuracy is a very common metric, but it is not always the best metric to use. For example, if we have a
  dataset with 99% of the observations belonging to class A and 1% belonging to class B, a classifier that always
  predicts class A will have an accuracy of 99%, even though it is a useless classifier. In this case, a better
  metric would be the precision or the recall.</p>
</div>
</li>
<li>
<p><strong>Confusion matrix</strong>: The confusion matrix is a table that shows the number of correct and incorrect predictions
   made by a classifier. It is defined as:</p>
<table>
<thead>
<tr>
<th></th>
<th>Predicted: 0</th>
<th>Predicted: 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actual: 0</td>
<td>True negatives</td>
<td>False positives</td>
</tr>
<tr>
<td>Actual: 1</td>
<td>False negatives</td>
<td>True positives</td>
</tr>
</tbody>
</table>
<p>The confusion matrix is implemented by all classifiers in Scikit Learn, and can be computed using the <code>confusion_matrix</code>
function in the <code>metrics</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a><span class="c1"># Output:</span>
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a><span class="p">[[</span><span class="mi">1</span> <span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a> <span class="p">[</span><span class="mi">1</span> <span class="mi">1</span><span class="p">]]</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The accuracy, precision, recall, etc. can all be computed from the confusion matrix.</p>
</div>
</li>
<li>
<p><strong>AUROC</strong>: The AUROC (Area Under the Receiver Operating Characteristic) is a metric that measures the performance
  of a classifier. It is defined as the area under the ROC curve, which is a curve that plots the true positive rate
  (TPR) against the false positive rate (FPR) at various threshold settings. The AUROC is a number between 0 and 1,
  where 1 means that the model is perfect and 0 means that the model is useless. The AUROC is implemented by all
  classifiers in Scikit Learn, and can be computed using the <code>roc_auc_score</code> function in the <code>metrics</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="c1"># Output:</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="mf">0.5</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The AUROC is a more robust metric than the accuracy, since it is not affected by the class imbalance problem
described above. </p>
</div>
</li>
</ul>
<h3 id="regression-metrics">Regression metrics<a class="headerlink" href="#regression-metrics" title="Permanent link">&para;</a></h3>
<p>Some of the most common regression metrics are:</p>
<ul>
<li>
<p><strong>Mean squared error</strong>: The mean squared error is the average squared difference between the predicted values and
  the true values of a regression model. It is defined as:</p>
<div class="arithmatex">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</div>
<p>where <span class="arithmatex">\(y_i\)</span> is the true value of the <span class="arithmatex">\(i\)</span>-th observation, <span class="arithmatex">\(\hat{y}_i\)</span> is the predicted value of the <span class="arithmatex">\(i\)</span>-th observation,
and <span class="arithmatex">\(n\)</span> is the number of observations. The MSE is implemented by all regressors in Scikit Learn, and can be often
computed using the <code>score</code> method.</p>
</li>
<li>
<p><strong>Mean absolute error</strong>: The mean absolute error is the average absolute difference between the predicted values and
    the true values of a regression model. It is defined as:
    $$
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
    $$
    where <span class="arithmatex">\(y_i\)</span> is the true value of the <span class="arithmatex">\(i\)</span>-th observation, <span class="arithmatex">\(\hat{y}_i\)</span> is the predicted value of the <span class="arithmatex">\(i\)</span>-th observation,
    and <span class="arithmatex">\(n\)</span> is the number of observations. The MAE is implemented by all regressors in Scikit Learn, and can be often
    computed using the <code>score</code> method.</p>
</li>
<li>
<p><strong><span class="arithmatex">\(R^2\)</span> score</strong>: The <span class="arithmatex">\(R^2\)</span> score is a metric that measures the proportion of variance in the dependent variable
    that is predictable from the independent variables. It is defined as:</p>
<p>$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y} _i)^2} 
{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$
where <span class="arithmatex">\(y_i\)</span> is the true value of the <span class="arithmatex">\(i\)</span>-th observation, <span class="arithmatex">\(\hat{y}_i\)</span> is the predicted value of the <span class="arithmatex">\(i\)</span>-th observation,
<span class="arithmatex">\(\bar{y}\)</span> is the mean of the true values, and <span class="arithmatex">\(n\)</span> is the number of observations. The <span class="arithmatex">\(R^2\)</span> score is a number between
0 and 1, where 1 means that the model is perfect and 0 means that the model is useless. The <span class="arithmatex">\(R^2\)</span> score is implemented
by all regressors in Scikit Learn, and can be often computed using the <code>score</code> method.</p>
</li>
</ul>
<h2 id="model-validation">Model validation<a class="headerlink" href="#model-validation" title="Permanent link">&para;</a></h2>
<p>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model 
that would just repeat the labels of the samples that it has just seen would have a perfect (or almost perfect) score 
but would fail to predict anything useful on yet-unseen data. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This situation is called <strong>overfitting</strong>. This happens when the model learns the training data "too well". Since 
all datasets contain some noise, eventually the model will fit to the noise as well, which will cause it to perform
worse on new data.</p>
</div>
<h3 id="holdout-sets-with-train_test_split">Holdout sets with <code>train_test_split</code><a class="headerlink" href="#holdout-sets-with-train_test_split" title="Permanent link">&para;</a></h3>
<p>To avoid this situation, it is common practice when performing a (supervised) machine learning experiment to hold out 
part of the available data. </p>
<p>In scikit-learn a random split into training and test sets can be quickly computed with the <code>train_test_split</code> 
helper function. Let's use it to hold out 25% of the data for testing, in the following example:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="c1"># fit the model on one set of data</span>
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a>
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a><span class="c1"># evaluate the model on the second set of data</span>
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a><span class="n">y2_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y2_model</span><span class="p">)</span>
</code></pre></div>
<h3 id="cross-validation">Cross-validation<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h3>
<p>One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the 
model training. In the above case, half the dataset does not contribute to the training of the model! This is 
not optimal, and can cause problems – especially if the initial set of training data is small.</p>
<p>One way to address this is to use cross-validation; that is, to do a sequence of fits where each subset of 
the data is used both as a training set and as a validation set. Visually, it might look something like this:</p>
<figure>
<p><img alt="cv" src="../../../images/cross_validation.png" width="500" />
  </p>
<figcaption>Sketch of the cross-validation procedure.</figcaption>
</figure>
<p>Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 
4/5 of the data. This would be rather tedious to do by hand, and so we can use Scikit-Learn's <code>cross_val_score</code> 
convenience routine to do it succinctly:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="c1"># Output:</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="n">array</span><span class="p">([</span><span class="mf">0.96666667</span><span class="p">,</span> <span class="mf">0.96666667</span><span class="p">,</span> <span class="mf">0.93333333</span><span class="p">,</span> <span class="mf">0.96666667</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
</code></pre></div>
<p>Repeating the validation across different subsets of the data gives us a better idea of the performance 
of the algorithm. What comes out are <span class="arithmatex">\(n\)</span> accuracy scores, which we could combine (for example, by taking the
median or the mean). </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When working with time series data, it is important to use cross-validation in a way that respects the time
dependence between observations. In this case, we can use the <code>TimeSeriesSplit</code> class instead to
perform cross-validation.</p>
</div>
<h2 id="model-hyperparameters">Model hyperparameters<a class="headerlink" href="#model-hyperparameters" title="Permanent link">&para;</a></h2>
<p>A model <strong>hyperparameter</strong> is a configuration that is external to the model and whose value cannot be estimated
from data. They represent the "knobs" of a model: the type of function used to fit the model, the number of parameters,
the regularization parameter, the network architecture (in the case of neural networks), etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hyperparameters are different from parameters. Parameters are values that are estimated from data, such as the
coefficients of a linear regression model, or the weights of a neural network. Hyperparameters are <em>everything else</em>.</p>
</div>
<p>Of core importance is the following question: if our estimator is underperforming, how should we move forward? 
There are several possible answers:</p>
<ul>
<li>Use a more complicated/more flexible model</li>
<li>Use a less complicated/less flexible model</li>
<li>Gather more training samples</li>
<li>Gather more data to add features to each sample</li>
</ul>
<p>The answer to this question is often counter-intuitive. Sometimes using a more complicated 
model will give worse results, and adding more training samples may not improve your results!</p>
<h3 id="bias-variance-trade-off">Bias-variance trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permanent link">&para;</a></h3>
<p>Fundamentally, the question of "the best model" is about finding a sweet spot in the tradeoff between bias and 
variance. Consider the following figure, which presents two regression fits to the same dataset:</p>
<figure>
<p><img alt="bias_variance" src="../../../images/bias_variance.png" width="700" />
  </p>
<figcaption>Two regression fits to the same dataset. Left: a high-bias, low-variance model; 
    Right: a low-bias, high-variance model.</figcaption>
</figure>
<p>In general, as we increase the number of tunable parameters in a model, it becomes more flexible, 
and can better fit a training data set. It is said to have lower error, or <strong>bias</strong>. However, for more 
flexible models, there will tend to be greater <strong>variance</strong> to the model fit each time we take a set of samples 
to create a new training data set.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>bias–variance trade-off</strong> is the conflict in trying to simultaneously minimize these two sources of error.</p>
</div>
<p>From the previous figure, we see that (and the observation generally holds):</p>
<ul>
<li>For <strong>high-bias</strong> models, the performance of the model on the validation set is similar to the performance 
  on the training set.</li>
<li>For <strong>high-variance</strong> models, the performance of the model on the validation set is far worse than the 
  performance on the training set.</li>
</ul>
<h3 id="validation-curves">Validation curves<a class="headerlink" href="#validation-curves" title="Permanent link">&para;</a></h3>
<p>If we imagine that we have some ability to tune the model complexity, we would expect the training score 
and validation score to behave as illustrated in the following figure:</p>
<figure>
<p><img alt="validation_curve" src="../../../images/validation_curve.png" width="500" />
  </p>
<figcaption>A **validation curve**.
    The X axis represents the model complexity (e.g., the number of neighbors in a KNN classifier, or a polynomial
    degree in a linear regression model). The Y axis represents the score (a higher score means better model 
    performance / lower error).</figcaption>
</figcaption>
</figure>
<p>The diagram above is a <strong>validation curve</strong>, and we see the following essential features:</p>
<ul>
<li>The training score is (almost) always better than the validation score. This is generally the case: 
    the model will be a better fit to data it has seen than to data it has not seen.</li>
<li>For very low model complexity (a high-bias model), the training data is <strong>under-fit</strong>, which means that 
  the model is a poor predictor both for the training data and for any previously unseen data.</li>
<li>For very high model complexity (a high-variance model), the training data is <strong>over-fit</strong>, which means 
that the model predicts the training data very well, but fails for any previously unseen data.</li>
<li>For some intermediate value, the validation curve has a maximum. This is the optimal scenario, and the parameter
  value set at the maximum is the best model.</li>
</ul>
<h3 id="learning-curves">Learning curves<a class="headerlink" href="#learning-curves" title="Permanent link">&para;</a></h3>
<p>One important aspect of model complexity is that the optimal model will generally depend on the size of your
training data. A <strong>learning Curve</strong> is a plot of the training and cross-validation error as a function of the
number of training samples. The general behavior we would expect from a learning curve is this:</p>
<ul>
<li>A model of a given complexity will overfit a small dataset: this means the training score will be relatively 
high, while the validation score will be relatively low.</li>
<li>A model of a given complexity will underfit a large dataset: this means that the training score will decrease, 
but the validation score will increase.</li>
<li>A model will never, except by chance, give a better score to the validation set than the training set: 
this means the curves should keep getting closer together but never cross.</li>
</ul>
<p>The following figure shows a typical learning curve for a supervised learning problem:</p>
<figure>
<p><img alt="validation_curve" src="../../../images/learning_curve.png" width="500" />
  </p>
<figcaption>A _typical_ learning curve.</figcaption>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Another common type of curve are <strong>Loss vs. epoch graphs</strong>, where in the Y axis we have a loss function 
to minimize (an "error") and on the X axis we have the number of epochs (iterations) of the training algorithm
(sort of like the time axis). These curves are used to monitor the training process of a model over time,
and to determine when to stop training. The following figure shows a loss vs. epoch graph for a neural network:</p>
<p><figure markdown>
  <img alt="validation_curve" src="../../../images/loss_epoch.png" width="500" />
  <figcaption>A <strong>loss vs. epoch graph</strong> example.</figcaption>
</figure></p>
</div>
<h3 id="cross-validation-including-hyperparameter-tuning">Cross-validation including hyperparameter tuning<a class="headerlink" href="#cross-validation-including-hyperparameter-tuning" title="Permanent link">&para;</a></h3>
<p>The preceding discussion of the validation and learning curves 
is meant to give you some intuition into the trade-off between bias and variance,
and its dependence on model complexity and training set size. In practice, models generally have more 
than one knob to turn, and thus plots of validation and learning curves change from lines to 
multi-dimensional surfaces. In these cases, such visualizations are difficult and we would rather 
simply find the particular model that maximizes the validation score.</p>
<p>Scikit Learn provides a number of tools to perform model selection, including cross-validation and grid search
on hyperparameters. A common way to do this is to use the <code>GridSearchCV</code> class, which implements a grid search 
with cross-validation. </p>
<p>This class takes as input an estimator, a dictionary of hyperparameters, and a cross-validation strategy, and returns
the best hyperparameters for the estimator. For example, to find the best number of neighbors for a KNN classifier,
we can do the following:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]}</span>
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a><span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a>
<a id="__codelineno-13-13" name="__codelineno-13-13" href="#__codelineno-13-13"></a><span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<a id="__codelineno-13-14" name="__codelineno-13-14" href="#__codelineno-13-14"></a><span class="nb">print</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>cv</code> argument is the number of folds in the cross-validation. The default value is 5, but it can be changed
to any integer.</p>
</div>
<p>Now that this is fit, we can ask for the best parameters as follows:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="c1"># Output:</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
</code></pre></div>
<p>The grid search provides other options, including the ability to specify a custom scoring function, 
to parallelize the computations, to do randomized searches, and more.</p>
<h2 id="feature-engineering">Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permanent link">&para;</a></h2>
<p>All of the examples so far assume that you have numerical data in a tidy, <code>[n_samples, n_features]</code> format. 
In the real world, data rarely comes in such a form. Hence, one of the more important steps in using machine 
learning in practice is <strong>feature engineering</strong>: that is, taking whatever information you have about your 
problem and turning it into numbers that you can use to build your feature matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Feature engineering is a fancy term for "creating new variables from existing variables". For example, if we have
a dataset with a column containing the date of birth of a person, we can create a new column containing the age of
the person by subtracting the date of birth from the current date.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Proper data cleaning is also an important part of feature engineering. Some common data cleaning tasks include:</p>
<ul>
<li><strong>Missing values</strong>: Most machine learning algorithms cannot handle missing values. There are several ways to deal
  with missing values, including removing the observations with missing values, imputing the missing values with the
  mean or median, or using a machine learning algorithm that can handle missing values.</li>
<li><strong>Outliers</strong>: Outliers are observations that are far away from the rest of the observations. Outliers can have a
  significant effect on the model, so it is important to detect and (if it is justified to do so) remove them. </li>
</ul>
</div>
<h3 id="types-of-features">Types of features<a class="headerlink" href="#types-of-features" title="Permanent link">&para;</a></h3>
<p>The following sections describe a few general tricks of feature engineering.</p>
<h4 id="categorical-features">Categorical features<a class="headerlink" href="#categorical-features" title="Permanent link">&para;</a></h4>
<p>One common type of non-numerical data is categorical data. 
To use this data in a machine learning model, we need to convert this categorical feature to a numerical feature.</p>
<p>If the categorical feature has no natural ordering, we can use one-hot encoding to convert it to a numerical
feature. If the categorical feature has a natural ordering, we can use ordinal encoding to convert it to a regular
number.</p>
<h4 id="text-features">Text features<a class="headerlink" href="#text-features" title="Permanent link">&para;</a></h4>
<p>Another common type of non-numerical data is text data. To use this data in a machine learning
model, we need to convert this text feature to a numerical feature. This can be done using several NLP techniques. 
Check <strong>bag-of-words</strong>, <strong>TF-IDF</strong>, and <strong>word embeddings</strong> for more information.</p>
<h4 id="date-features">Date features<a class="headerlink" href="#date-features" title="Permanent link">&para;</a></h4>
<p>Another common type of non-numerical data is date data. To use this data in a machine learning
model, we need to convert this date feature to a numerical feature. This can be done by extracting the year, month,
day, etc. from the date and using them as numerical features. </p>
<p>Date features are very important in time series, which naturally has seasonality and trends. For this, 
sometimes it is a good idea to use extra dummy variables to be able to
express all dates that are close to each other (e.g., the day of the week, the number of the week, etc.). These extra
variables should be the same as the regular ones, but with a different origin. </p>
<p>For example, if we only have the week
number as a feature, we might not be able to express that week 52 and week 1 are close to each other. However, if we
<em>also</em> add a new feature that is the week number plus 26, the model might be able to understand that weeks
"live" in a circle instead of a line.</p>
<h4 id="image-features">Image features<a class="headerlink" href="#image-features" title="Permanent link">&para;</a></h4>
<p>Another common type of data are images. Although they might look like non-numerical data, images are actually
matrices of numbers. To use this data in a machine learning, the easiest way is to flatten the image and use the
pixels as numerical features (often even deleting the color channels).</p>
<h4 id="derived-features">Derived features<a class="headerlink" href="#derived-features" title="Permanent link">&para;</a></h4>
<p>Sometimes, we can create new features from existing features. For example, if we have a dataset with a column
containing the date of birth of a person, we can create a new column containing the age of the person by subtracting
the date of birth from the current date. </p>
<p>For numerical data, we can often apply mathematical transformations to create new features that have a better
behavior for the model, or that change the data distribution to one that is more suitable. This needs to be done
on a case-by-case basis, and requires some domain knowledge.</p>
<h3 id="feature-selection">Feature selection<a class="headerlink" href="#feature-selection" title="Permanent link">&para;</a></h3>
<p>In some cases, we might have too many features, which makes the model too complex and costly to maintain. In such cases
we might want to select only the most important ones. This can be achieved in several ways:</p>
<ul>
<li>With dimensionality reduction techniques, such as <strong>PCA</strong>.</li>
<li>With feature importance techniques, such as the <strong>SHAP values</strong> (and simpler alternatives). A good rule of thumb to
know if a feature is important is to add noise to it and see if the model performance decreases. If it does, the
feature is probably important. Another, similar, option is to add random noise features and scrap all features
that perform worse than the random noise features.</li>
</ul>
<h2 id="machine-learning-algorithms-in-sklearn">Machine learning algorithms in SKLearn<a class="headerlink" href="#machine-learning-algorithms-in-sklearn" title="Permanent link">&para;</a></h2>
<p>Scikit Learn provides implementations of a large number of machine learning algorithms. These algorithms are
implemented as the already discussed estimator objects, and can be used to solve a wide range of machine 
learning problems.</p>
<h3 id="regression-models">Regression models<a class="headerlink" href="#regression-models" title="Permanent link">&para;</a></h3>
<p>Regression is the task of predicting a continuous value. For example, predicting the price of a house is a
regression task, or predicting the height of a person. Scikit Learn provides a number of regression models that
can be used to solve regression problems.</p>
<h4 id="common-regression-models">Common regression models<a class="headerlink" href="#common-regression-models" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Linear regression</strong>: Linear regression is a linear model that assumes a linear relationship between the
  dependent variable and the independent variables. It is implemented by the <code>LinearRegression</code> class in the
  <code>linear_model</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p><strong>Random forest regression</strong>: Random forest regression is an ensemble model that fits a number of decision tree
  classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control
  over-fitting. It is implemented by the <code>RandomForestRegressor</code> class in the <code>ensemble</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
</li>
</ul>
<h4 id="regression-examples">Regression examples<a class="headerlink" href="#regression-examples" title="Permanent link">&para;</a></h4>
<p>https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Supervised-learning-example:-Simple-linear-regression</p>
<h3 id="classification">Classification<a class="headerlink" href="#classification" title="Permanent link">&para;</a></h3>
<p>Classification is the task of predicting a class from a set of classes. For example, predicting whether an email is
spam or not is a classification task, or predicting whether a person has a disease or not. Scikit Learn provides a
number of classification models that can be used to solve classification problems.</p>
<h4 id="common-classification-models">Common classification models<a class="headerlink" href="#common-classification-models" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Logistic regression</strong>: Logistic regression is a linear model that assumes a linear relationship between the
  log-odds of the dependent variable and the independent variables. It is implemented by the <code>LogisticRegression</code>
  class in the <code>linear_model</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Despite its name, logistic regression is a classification algorithm, not a regression algorithm. The name
comes from the fact that it is a regression algorithm that uses logistic functions to model the relationship
between the dependent and independent variables. It can be used for binary classification problems or
multi-class classification problems (i.e., problems with more than two classes).</p>
</div>
</li>
<li>
<p><strong>K-Nearest Neighbors</strong>: K-Nearest Neighbors is a non-parametric method used for classification and regression. 
  It is a lazy learning algorithm that does not fit a model to the data. It is implemented by the <code>KNeighborsClassifier</code>
  class in the <code>neighbors</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="n">y_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p><strong>Random forest classification</strong>: Random forest classification is an ensemble model that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and
    control over-fitting. It is implemented by the <code>RandomForestClassifier</code> class in the <code>ensemble</code> module.</p>
<div class="highlight"><pre><span></span><code>```python
from sklearn.ensemble import RandomForestClassifier

X_train = ...
y_train = ...

model = RandomForestClassifier()
model.fit(X_train, y_train)
```
</code></pre></div>
</li>
</ul>
<h4 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h4>
<p>https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Supervised-learning-example:-Iris-classification</p>
<h3 id="clustering">Clustering<a class="headerlink" href="#clustering" title="Permanent link">&para;</a></h3>
<p>Clustering is the task of grouping data points into clusters. For example, grouping customers into clusters based on
their purchase history is a clustering task. Scikit Learn provides a number of clustering algorithms that can be used
to solve clustering problems.</p>
<h4 id="common-clustering-models">Common clustering models<a class="headerlink" href="#common-clustering-models" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>K-Means</strong>: K-Means is a clustering algorithm that partitions the data into K clusters. It is implemented by the
  <code>KMeans</code> class in the <code>cluster</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a><span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div>
</li>
<li>
<p><strong>HD-BSCAN</strong>: HD-BSCAN is a clustering algorithm that partitions the data into clusters. It is implemented by the
  <code>HDBSCAN</code> class in the <code>cluster</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">HDBSCAN</span>
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>
<a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a><span class="n">model</span> <span class="o">=</span> <span class="n">HDBSCAN</span><span class="p">()</span>
<a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div>
</li>
</ul>
<h4 id="examples_1">Examples<a class="headerlink" href="#examples_1" title="Permanent link">&para;</a></h4>
<p>https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Unsupervised-learning:-Iris-clustering</p>
<h3 id="dimensionality-reduction">Dimensionality reduction<a class="headerlink" href="#dimensionality-reduction" title="Permanent link">&para;</a></h3>
<p>Dimensionality reduction is the task of reducing the number of features in a dataset. For example, reducing the number
of features in an image to the most important ones is a dimensionality reduction task. Scikit Learn provides a number
of dimensionality reduction algorithms that can be used to solve dimensionality reduction problems.</p>
<h4 id="common-dimensionality-reduction-models">Common dimensionality reduction models<a class="headerlink" href="#common-dimensionality-reduction-models" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Principal Component Analysis</strong>: Principal Component Analysis (PCA) is a dimensionality reduction algorithm that
  transforms the data into a lower-dimensional space. It is implemented by the <code>PCA</code> class in the <code>decomposition</code> module.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="n">X_train</span> <span class="o">=</span> <span class="o">...</span>
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>
<a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Not all dimensionality reduction algorithms can be found in SKLearn. UMAP, for example, is a very popular
dimensionality reduction algorithm that is not implemented.</p>
</div>
<h4 id="examples_2">Examples<a class="headerlink" href="#examples_2" title="Permanent link">&para;</a></h4>
<p>https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Unsupervised-learning-example:-Iris-dimensionality</p>
<h2 id="annex-common-pitfalls-in-ml">Annex: common pitfalls in ML<a class="headerlink" href="#annex-common-pitfalls-in-ml" title="Permanent link">&para;</a></h2>
<h3 id="curse-of-dimensionality">Curse of dimensionality<a class="headerlink" href="#curse-of-dimensionality" title="Permanent link">&para;</a></h3>
<p>The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in 
high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical 
space of everyday experience. </p>
<p>In machine learning, the curse of dimensionality is often used to refer to the fact that the performance of many
machine learning algorithms degrades as the number of features increases. This is because, as the number of features
increases, the number of observations required to obtain a good model increases exponentially with the number of
features.</p>
<h3 id="overfitting">Overfitting<a class="headerlink" href="#overfitting" title="Permanent link">&para;</a></h3>
<p>Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.
Overfitting the model generally takes the form of making an overly complex model to explain idiosyncrasies in the data
under study. In reality, the data being studied often has some degree of error or random noise within it. Thus,
attempting to make the model conform too closely to slightly inaccurate data can infect the model with substantial
errors and reduce its predictive power.</p>
<h3 id="underfitting">Underfitting<a class="headerlink" href="#underfitting" title="Permanent link">&para;</a></h3>
<p>Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the
data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough. More 
specifically, underfitting occurs if the model or algorithm shows low variance but high bias. 
Underfitting is often a result of an excessively simple model.</p>
<h3 id="data-leakage">Data leakage<a class="headerlink" href="#data-leakage" title="Permanent link">&para;</a></h3>
<p>Data leakage is a problem that occurs when information about the target variable is inadvertently introduced into the
training data. This can cause the model to perform unrealistically well during training, but perform poorly during
testing.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we are not careful with time series data when splitting the data into training and test sets, we can introduce 
data leakage by using information from the future to predict the past!    </p>
</div>
<h3 id="class-imbalance">Class imbalance<a class="headerlink" href="#class-imbalance" title="Permanent link">&para;</a></h3>
<p>Class imbalance is a problem that occurs when the number of observations in each class is not equal. This can cause
problems when training a model, since the model will tend to predict the most common class. For example, if we have
a dataset with 99% of the observations belonging to class A and 1% belonging to class B, a classifier that always
predicts class A will have an accuracy of 99%, even though it is a useless classifier. In this case, a better
metric would be the AUROC.</p>
<h2 id="other">Other<a class="headerlink" href="#other" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Clustering</strong>: Clustering is the task of grouping data points into clusters. For example, grouping customers into
  clusters based on their purchase history is a clustering task.</li>
<li><strong>Dimensionality reduction</strong>: Dimensionality reduction is the task of reducing the number of features in a dataset.
  For example, reducing the number of features in an image to the most important ones is a dimensionality reduction
  task.</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.ba449ae6.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>